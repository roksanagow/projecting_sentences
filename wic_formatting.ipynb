{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19582306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "from collections import Counter\n",
    "import random\n",
    "import math \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d56d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_test_dev_sentences(test_dev_sentences_df, seed):\n",
    "    ''' Take an equal amount of sentences from each sense for each worf for test and dev sets'''\n",
    "    test_sentences, dev_sentences = pd.DataFrame(), pd.DataFrame()\n",
    "    for word in test_dev_sentences_df.lemma.unique():\n",
    "        word_df = test_dev_sentences_df[test_dev_sentences_df.lemma == word]\n",
    "        for sense in word_df.sense.unique():\n",
    "            sense_df = word_df[word_df.sense == sense]\n",
    "            test_samples = sense_df.sample(frac=0.5, random_state=seed)\n",
    "            dev_samples = sense_df[~sense_df.index.isin(test_samples.index)]\n",
    "            test_sentences = pd.concat([test_sentences, test_samples], ignore_index=True)\n",
    "            dev_sentences = pd.concat([dev_sentences, dev_samples], ignore_index=True)\n",
    "\n",
    "    return test_sentences, dev_sentences\n",
    "\n",
    "def numpy_encoder(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return str(obj)  # Convert any other unknown type to string\n",
    "\n",
    "# Sample test and dev sentences\n",
    "def sample_sentences(word_list, wsd_df):\n",
    "    train_sentences, test_dev_sentences = pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    for word in word_list:\n",
    "        word_df = wsd_df[wsd_df.lemma == word]\n",
    "        \n",
    "        # Get all unique labels for the current word\n",
    "        labels = word_df.sense.unique()\n",
    "        \n",
    "        # Initialize temporary DataFrames for this word's samples\n",
    "        train_samples = pd.DataFrame()\n",
    "        test_dev_samples = pd.DataFrame()\n",
    "        \n",
    "        for label in labels:\n",
    "            # Filter DataFrame by the current label\n",
    "            label_df = word_df[word_df.sense == label]\n",
    "            \n",
    "            # Shuffle the rows\n",
    "            sampled_df = label_df.sample(frac=1.0, random_state=42)  # random_state for reproducibility\n",
    "            train_split_index = int(len(sampled_df) * 0.75)\n",
    "            \n",
    "            # Split into train and test/dev sets\n",
    "            train_samples_label = sampled_df.iloc[:train_split_index]\n",
    "            test_dev_samples_label = sampled_df.iloc[train_split_index:]\n",
    "            \n",
    "            # Append the label-specific samples to the respective DataFrames\n",
    "            train_samples = pd.concat([train_samples, train_samples_label], ignore_index=True)\n",
    "            test_dev_samples = pd.concat([test_dev_samples, test_dev_samples_label], ignore_index=True)\n",
    "        \n",
    "        # Concatenate the word-specific samples to the final DataFrames\n",
    "        train_sentences = pd.concat([train_sentences, train_samples], ignore_index=True)\n",
    "        test_dev_sentences = pd.concat([test_dev_sentences, test_dev_samples], ignore_index=True)\n",
    "\n",
    "    return train_sentences, test_dev_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2778b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word(word_df, seed, repeat_num):\n",
    "    for i in range(repeat_num):\n",
    "        word_df[f'same_sense_paired_{i}'] = None\n",
    "        word_df[f'diff_sense_paired_{i}'] = None\n",
    "\n",
    "    word_df = word_df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    # Pair up sentences with the same sense sentences\n",
    "    for sense in word_df.sense.unique():\n",
    "        sense_df = word_df[word_df.sense == sense] #.reset_index(drop=True) #.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "        # Iterate for each repeat number\n",
    "        for i in range(repeat_num):\n",
    "            # Loop through each sentence in the sense-specific DataFrame\n",
    "            for row1 in sense_df.itertuples():\n",
    "                # Dynamically check up to same_sense_paired_{i-1}\n",
    "                if pd.isna(getattr(row1, f'same_sense_paired_{i}')):\n",
    "                    # List of columns to check for previous pairings\n",
    "                    previous_columns = [f'same_sense_paired_{j}' for j in range(i)]\n",
    "                    \n",
    "                    # Filter unpaired candidates for same_sense\n",
    "                    unpaired_candidates = sense_df[\n",
    "                        (sense_df[f'same_sense_paired_{i}'].isna()) &  # Ensure this column is NaN\n",
    "                        (~sense_df[previous_columns].eq(row1.Index, axis=0).any(axis=1)) &  # No prior pairing in same_sense_paired_0..i-1\n",
    "                        (sense_df.index != row1.Index)  # Exclude the current sentence\n",
    "                    ]\n",
    "                    if not unpaired_candidates.empty:\n",
    "                        # Randomly select one candidate for pairing\n",
    "                        row2 = unpaired_candidates.sample(1).iloc[0]\n",
    "\n",
    "                        # Update the DataFrame for the current pairing\n",
    "                        word_df.at[row1.Index, f'same_sense_paired_{i}'] = row2.name\n",
    "                        word_df.at[row2.name, f'same_sense_paired_{i}'] = row1.Index\n",
    "                        \n",
    "                        # Update sense-specific DataFrame as well\n",
    "                        sense_df.at[row1.Index, f'same_sense_paired_{i}'] = row2.name\n",
    "                        sense_df.at[row2.name, f'same_sense_paired_{i}'] = row1.Index\n",
    "\n",
    "    # Initialize diff_sense_paired columns for each repeat\n",
    "    for i in range(repeat_num):\n",
    "        word_df[f'diff_sense_paired_{i}'] = None\n",
    "\n",
    "    # Sense counts and inter-set pair tracking\n",
    "    sense_counts = word_df.sense.value_counts().to_dict()\n",
    "    sense_pair_counts = {sense: {other_sense: 0 for other_sense in word_df.sense.unique()} for sense in word_df.sense.unique()}\n",
    "\n",
    "    # Iterate over each repeat number to pair sentences with different senses\n",
    "    for i in range(repeat_num):\n",
    "        for row_index in range(len(word_df)):\n",
    "            row = word_df.iloc[row_index]\n",
    "            # If the current diff_sense_paired_{i} column is NaN\n",
    "            if pd.isna(row[f'diff_sense_paired_{i}']):\n",
    "                row_sense = row.sense\n",
    "\n",
    "                # Find sense with the fewest inter-set pairs, relative to the target proportions\n",
    "                sense_ratios = {\n",
    "                    sense: sense_pair_counts[row_sense][sense] / sense_counts[sense]\n",
    "                    for sense in sense_counts if sense != row_sense\n",
    "                }\n",
    "\n",
    "                if sense_ratios:\n",
    "                    # Select the sense with the lowest pairing ratio to have the right proportion of label pairs\n",
    "                    less_than_proportion_sense = min(sense_ratios, key=sense_ratios.get)\n",
    "\n",
    "                    # Get candidate sentences from the selected sense\n",
    "                    candidate_sentences = word_df[\n",
    "                        (word_df.sense == less_than_proportion_sense) & # Select the sense with the lowest pairing ratio\n",
    "                        pd.isna(word_df[f'diff_sense_paired_{i}']) &  # Ensure the current column is unpaired\n",
    "                        (~word_df[[f'diff_sense_paired_{j}' for j in range(i)]].eq(row_index, axis=0).any(axis=1)) &  # Ensure no previous pairing with this sentence\n",
    "                        (word_df.index != row.name)  # Exclude the current sentence\n",
    "                    ]\n",
    "\n",
    "                    # Check for candidates and select one randomly\n",
    "                    if not candidate_sentences.empty:\n",
    "                        diff_sense_sentence = candidate_sentences.sample(1).iloc[0].name\n",
    "                        word_df.at[row.name, f'diff_sense_paired_{i}'] = diff_sense_sentence\n",
    "                        word_df.at[diff_sense_sentence, f'diff_sense_paired_{i}'] = row.name\n",
    "\n",
    "                        # Update the sense pair counts\n",
    "                        sense_pair_counts[row_sense][less_than_proportion_sense] += 1\n",
    "                        sense_pair_counts[less_than_proportion_sense][row_sense] += 1\n",
    "\n",
    "                else:\n",
    "                    # If all other senses have reached the target proportions, pair with any different sense\n",
    "                    candidate_sentences = word_df[\n",
    "                        (word_df.sense != row_sense) &\n",
    "                        pd.isna(word_df[f'diff_sense_paired_{i}']) &\n",
    "                        (~word_df[[f'diff_sense_paired_{j}' for j in range(i)]].eq(row_index, axis=0).any(axis=1)) &\n",
    "                        (word_df.index != row.name)\n",
    "                    ]\n",
    "\n",
    "                    if not candidate_sentences.empty:\n",
    "                        diff_sense_sentence = candidate_sentences.sample(1).iloc[0].name\n",
    "                        word_df.at[row.name, f'diff_sense_paired_{i}'] = diff_sense_sentence\n",
    "                        word_df.at[diff_sense_sentence, f'diff_sense_paired_{i}'] = row.name\n",
    "\n",
    "                        # Update the sense pair counts\n",
    "                        sense_pair_counts[row_sense][word_df.loc[diff_sense_sentence].sense] += 1\n",
    "                        sense_pair_counts[word_df.loc[diff_sense_sentence].sense][row_sense] += 1\n",
    "\n",
    "    return word_df\n",
    "                    \n",
    "def extract_sentence_pairs(word_df, repeat_num):\n",
    "    # Initialize lists to store the extracted data\n",
    "    sentence_pairs, id1s, id2s, labels = [], [], [], []\n",
    "    start1s, end1s, start2s, end2s, lemmas = [], [], [], [], []\n",
    "    \n",
    "    # Use a set to track pairs that have been added to avoid duplicates\n",
    "    added_pairs = set()\n",
    "\n",
    "    for i in range(repeat_num):\n",
    "\n",
    "        # Iterate over each row in the DataFrame\n",
    "        for _, row in word_df.iterrows():\n",
    "            # Process same_sense_paired pairs\n",
    "            if pd.notna(row[f'same_sense_paired_{i}']):\n",
    "                paired_row = word_df.loc[row[f'same_sense_paired_{i}']]\n",
    "                pair = (row['sentence'], paired_row['sentence'])\n",
    "                reverse_pair = (paired_row['sentence'], row['sentence'])\n",
    "                \n",
    "                # Only add the pair if it or its reverse hasn't been added already\n",
    "                if pair not in added_pairs and reverse_pair not in added_pairs:\n",
    "                    # Append data to lists for same-sense pair\n",
    "                    sentence_pairs.append((row['sentence'], paired_row['sentence']))\n",
    "                    id1s.append(row['sent_id'])\n",
    "                    id2s.append(paired_row['sent_id'])\n",
    "                    labels.append(1)  # Label 1 for same sense\n",
    "                    start1s.append(row['start'])\n",
    "                    end1s.append(row['end'])\n",
    "                    start2s.append(paired_row['start'])\n",
    "                    end2s.append(paired_row['end'])\n",
    "                    lemmas.append(row['lemma'])\n",
    "                    \n",
    "                    # Add the pair to the set to mark it as added\n",
    "                    added_pairs.add(pair)\n",
    "\n",
    "    for i in range(repeat_num):\n",
    "        for _, row in word_df.iterrows():\n",
    "            # Process diff_sense_paired pairs\n",
    "            if pd.notna(row[f'diff_sense_paired_{i}']):\n",
    "                paired_row = word_df.loc[row[f'diff_sense_paired_{i}']]\n",
    "                pair = (row['sentence'], paired_row['sentence'])\n",
    "                reverse_pair = (paired_row['sentence'], row['sentence'])\n",
    "\n",
    "                # Only add the pair if it or its reverse hasn't been added already\n",
    "                if pair not in added_pairs and reverse_pair not in added_pairs:\n",
    "                    # Append data to lists for different-sense pair\n",
    "                    sentence_pairs.append((row['sentence'], paired_row['sentence']))\n",
    "                    id1s.append(row['sent_id'])\n",
    "                    id2s.append(paired_row['sent_id'])\n",
    "                    labels.append(0)  # Label 0 for different sense\n",
    "                    start1s.append(row['start'])\n",
    "                    end1s.append(row['end'])\n",
    "                    start2s.append(paired_row['start'])\n",
    "                    end2s.append(paired_row['end'])\n",
    "                    lemmas.append(row['lemma'])\n",
    "                    \n",
    "                    # Add the pair to the set to mark it as added\n",
    "                    added_pairs.add(pair)\n",
    "\n",
    "    # Format the data into a list of dictionaries\n",
    "    formatted_data = [\n",
    "        {\n",
    "            \"lemma\": lemma,\n",
    "            \"sentence1\": pair[0],\n",
    "            \"sentence2\": pair[1],\n",
    "            \"sent_id1\": sent_id1,\n",
    "            \"sent_id2\": sent_id2,\n",
    "            \"start1\": start1,\n",
    "            \"end1\": end1,\n",
    "            \"start2\": start2,\n",
    "            \"end2\": end2,\n",
    "            \"label\": label\n",
    "        }\n",
    "        for pair, label, start1, end1, start2, end2, lemma, sent_id1, sent_id2 \n",
    "        in zip(sentence_pairs, labels, start1s, end1s, start2s, end2s, lemmas, id1s, id2s)\n",
    "    ]\n",
    "    \n",
    "    return formatted_data\n",
    "\n",
    "# Helper function to process sentences for each word\n",
    "def create_split(wsd_df, word_list, output_file, seed=None, extra_sentences_df=None, repeat_num=1):\n",
    "    formatted_data = []\n",
    "    \n",
    "    for word in word_list:\n",
    "        word_df = wsd_df[wsd_df.lemma == word]\n",
    "        new_word_df = process_word(word_df, seed, repeat_num)\n",
    "        formatted_data.extend(extract_sentence_pairs(new_word_df, repeat_num))\n",
    "\n",
    "    # Include additional sentences if provided\n",
    "    if extra_sentences_df is not None:\n",
    "        for word in extra_sentences_df.lemma.unique():\n",
    "            word_df = extra_sentences_df[extra_sentences_df.lemma == word]\n",
    "            new_word_df = process_word(word_df, seed, repeat_num)\n",
    "            formatted_data.extend(extract_sentence_pairs(new_word_df, repeat_num))\n",
    "\n",
    "    label_counts = Counter([pair['label'] for pair in formatted_data])\n",
    "    counts_0 = label_counts[0]\n",
    "    counts_1 = label_counts[1]\n",
    "    sum = counts_0 + counts_1\n",
    "    print(f\"Label distribution 0:1 :\", round(counts_0/sum,2), round(counts_1/sum,2))\n",
    "\n",
    "    # Save the data\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(formatted_data, f, ensure_ascii=False, indent=4, default=numpy_encoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e59a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "short_langs = {\n",
    "    'Azerbaijani': 'az',\n",
    "    'Telugu': 'te',\n",
    "    'Marathi': 'mr',\n",
    "    'Swahili': 'sw',\n",
    "    'Vietnamese': 'vi',\n",
    "    'Punjabi': 'pa',\n",
    "    'Polish': 'pl',\n",
    "    'Urdu': 'ur',\n",
    "    'Korean': 'ko',\n",
    "    'Kannada': 'kn',\n",
    "    'Bulgarian': 'bg',\n",
    "}\n",
    "languages = short_langs.keys()\n",
    "\n",
    "seed = 0 # for random sampling\n",
    "# Set the maximum number that each sentence can be repeated (across unique pairs)\n",
    "repeat_num = 8\n",
    "\n",
    "for language in languages:\n",
    "    print()\n",
    "    print(language)\n",
    "    # Load data and initialize variables\n",
    "    wsd_df = pd.read_csv(f'./formatted_wsd_files/{language}.csv')\n",
    "    print(\"total sentences\", len(wsd_df))\n",
    "    # to each sentence, add a sentence id consisting of {short_langs[language]}_{sense}_{index in the dataframe}\n",
    "    # drop nan rows and print them\n",
    "    wsd_df = wsd_df.dropna()\n",
    "    \n",
    "    wsd_df['sent_id'] = wsd_df.apply(lambda x: f\"{short_langs[language]}_{int(x.sense)}_{x.name}\", axis=1)\n",
    "\n",
    "    words = wsd_df.lemma.unique()\n",
    "    print(\"num of words\", len(words))\n",
    "\n",
    "    # Split words into train, test, and dev sets\n",
    "    six = math.ceil(len(words) * 0.7)\n",
    "    two = math.ceil(len(words) * 0.15)\n",
    "\n",
    "    train_words = np.random.choice(words, six, replace=False)\n",
    "    dev_words = np.random.choice([word for word in words if word not in train_words], two, replace=False)\n",
    "    test_words = [word for word in words if word not in train_words and word not in dev_words]\n",
    "\n",
    "    percent_train_words = np.random.choice(train_words, math.ceil(len(train_words) * 0.3), replace=False)\n",
    "\n",
    "    # Generate and save datasets\n",
    "    train_sentences_df, test_dev_sentences_df = sample_sentences(percent_train_words, wsd_df)\n",
    "    excluded_train_words = train_sentences_df.lemma.unique()\n",
    "    train_words = [word for word in train_words if word not in excluded_train_words]\n",
    "\n",
    "    test_sentences_df, dev_sentences_df = divide_test_dev_sentences(test_dev_sentences_df, seed)\n",
    "\n",
    "    print(\"Creating train data...\")\n",
    "    create_split(wsd_df, train_words, f'./formatted_wic_files_{repeat_num*2}_reps/{language}_train.data', seed, train_sentences_df, repeat_num)\n",
    "\n",
    "    print(\"Creating test data...\")\n",
    "    create_split(wsd_df, test_words, f'./formatted_wic_files_{repeat_num*2}_reps/{language}_test.data', seed, test_sentences_df,repeat_num)\n",
    "\n",
    "    print(\"Creating dev data...\")\n",
    "    create_split(wsd_df, dev_words, f'./formatted_wic_files_{repeat_num*2}_reps/{language}_dev.data', seed, dev_sentences_df,repeat_num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-1 (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
